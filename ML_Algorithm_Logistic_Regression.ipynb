{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475414fb",
   "metadata": {},
   "source": [
    "## Implement logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f60087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e93c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x, y, iterations=100, learning_rate=0.01):\n",
    "    '''\n",
    "    m is no. of data points\n",
    "    n is no. of features\n",
    "    '''\n",
    "    m, n = len(x), len(x[0])\n",
    "    ## Beta_0 out of beta_other, beta_0 has a different form of gradient than other betas\n",
    "    beta_0, beta_other = initialize_params(n)\n",
    "    for _ in range(iterations):\n",
    "        gradient_beta_0, gradient_beta_other = (\n",
    "        compute_gradients(x, y, beta_0, beta_other, m, n, 50)\n",
    "        )\n",
    "        beta_0, beta_other = update_params(\n",
    "        beta_0, beta_other, gradient_beeta_0, gradient_beta_other, learning_rate\n",
    "        )\n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5208f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def initialize_params(n):\n",
    "    beta_0 = 0\n",
    "    beta_other = [random.random() for _ in range(n)]\n",
    "    return beta_0, beta_other\n",
    "\n",
    "def compute_gradients(x, y, beta_0, beta_other, m, n):\n",
    "    gradient_beta_0 = 0\n",
    "    gradietn_beta_other = [0] * n\n",
    "    \n",
    "    for i, point in enumerate(x):\n",
    "        pred = logistic_function(point, beta_0, beta_other)\n",
    "        for j, feature in enumerate(point):\n",
    "            gradient_beta_other[j] += (pred - y[i]) * feature / m # normalize them by the no. of data points n\n",
    "        gradient_beta_0 += (pred - y[i]) / m\n",
    "    return gradient_beta_0, gradient_beta_other\n",
    "\n",
    "def logistic_function_np(point, beta_0, beta_other):\n",
    "    return 1/(1 + np.exp(-(beta_0 + point.doc(beta_other))))\n",
    "\n",
    "def update_params(beta_0, beta_other, gradient_beta_0, gradient_beta_other, learning_rate):\n",
    "    # Notice the sign. Depending on how to calculate loss. \n",
    "    # derror_dy = pred_y[i] when overestimated, gradient is a positive value, need to subtract.\n",
    "    # beta_new = beta_old - positive gradient\n",
    "    # derro_dy = y]i] - pred\n",
    "    # beta_new = beta_old + positive gradient\n",
    "    beta_0 -= gradient_beta_0 * learning_rate\n",
    "    for i in range(len(beta_other)):\n",
    "        beta_other[i] -= (gradient_beta_other[i] * learning_rate)\n",
    "    return beta_0, beta_other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fab4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity\n",
    "\n",
    "## Time complexity O(MNI)\n",
    "## Space complexity O(N) # store gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b840b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization to be done\n",
    "\n",
    "## Mini-batch gradietn descent\n",
    "## Batch gradient desceint\n",
    "### loop through all the data -> gradeint\n",
    "### Has problem when dataset is too large\n",
    "\n",
    "## Advantage\n",
    "### smaller data size\n",
    "### Faster computation\n",
    "## Downside\n",
    "### Gradient is noisier\n",
    "\n",
    "## In practice\n",
    "### Very commonly used\n",
    "### Fast to get approriate\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradients(x, y, beta_0, beta_other, m, n, batch_size):\n",
    "    gradient_beta_0 = 0\n",
    "    gradietn_beta_other = [0] * n\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        i = random.randint(0, m-1)\n",
    "        point = x[i]\n",
    "        pred = logistic_function(point, beta_0, beta_other)\n",
    "    \n",
    "\n",
    "        for j, feature in enumerate(point):\n",
    "            gradient_beta_other[j] += (pred - y[i]) * feature / m # normalize them by the no. of data points n\n",
    "        gradient_beta_0 += (pred - y[i]) / m\n",
    "    return gradient_beta_0, gradient_beta_other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
